{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c40a557",
   "metadata": {},
   "source": [
    "# Project Overview Notebook\n",
    "\n",
    "**Note:** This notebook is **not intended to run the full training loop** or to rebuild all models from scratch.  \n",
    "Its purpose is to provide the reader with a **clear understanding of the repository workflow**, the **datasets**, and our **approach to solving the problem statement**.  \n",
    "\n",
    "It serves as a guided explanation of what has been done so far, including preprocessing steps, model design, and evaluation, without executing the heavy computations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2616ff5e",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "For this project, we use the **Ego4D dataset**, a large-scale dataset consisting of **long first-person videos** along with **narrations describing the events happening in the videos**.  \n",
    "\n",
    "- The dataset is **terabyte-scale**, making it **infeasible to use the full video data** given computational and storage constraints.  \n",
    "- Our approach does **not require video data during the initial stages**; videos are only used at the inference stage.  \n",
    "\n",
    "### What we are using:\n",
    "- **File:** `em_train_narrations.pkl` (located in the `data/` directory of the repository)  \n",
    "- This file contains all narrations from the dataset.  \n",
    "- We use these narrations to:\n",
    "  1. **Create a question-answer dataset** for our task.  \n",
    "  2. Include the **temporal grounding annotations**, which serve as ground truth for training.  \n",
    "\n",
    "By focusing on the narrations first, we can efficiently **train our model** without handling the massive video files until necessary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25f7955",
   "metadata": {},
   "source": [
    "## Sample Narrations from the Dataset\n",
    "\n",
    "In this snippet, we load the `em_train_narrations.pkl` file and display the narrations for **one video clip**.  \n",
    "\n",
    "- The dataset contains a total of **10,777 videos**, each with multiple narrations describing the events occurring in the video.  \n",
    "- For demonstration, we selected one video and printed **all its narrations** (instead of just the first 5) to show the type of information available.  \n",
    "- Each narration entry contains:\n",
    "  - `narration_text`: a textual description of an event.\n",
    "  - `timestamps`: the start and end times (in seconds) of the event within the video.  \n",
    "\n",
    "**Note:** In the narrations, **'C' represents the person performing the task or appearing in the video**, while **'O' represents another person**.  \n",
    "These narrations form the basis for creating **question-answer pairs** in our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d5d782b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of clips: 10777\n",
      "\n",
      "Clip ID: 30723771-7198-49cd-a5e3-ae413c904490\n",
      "Narration Pass: narration_pass_1\n",
      "Number of narrations in this clip: 13\n",
      "\n",
      "Narration 1: C opens a door to the garage with his right hand. .\n",
      "Timestamps: [0.2458468333333332, 9.45211]\n",
      "\n",
      "Narration 2: C closes the door with his right hand. .\n",
      "Timestamps: [5.30399, 14.510253166666663]\n",
      "\n",
      "Narration 3: C walks towards a bicycle parked outside. .\n",
      "Timestamps: [9.824106833333335, 19.08162]\n",
      "\n",
      "Narration 4: C mounts the bicycle. .\n",
      "Timestamps: [14.88225, 24.13976316666667]\n",
      "\n",
      "Narration 5: C rides the bicycle along the road. .\n",
      "Timestamps: [22.042896833333334, 32.159183166666665]\n",
      "\n",
      "Narration 6: C turns the bicycle right. .\n",
      "Timestamps: [28.91194683333333, 36.35805]\n",
      "\n",
      "Narration 7: C rides the bicycle along the road. .\n",
      "Timestamps: [33.97009, 41.416193166666666]\n",
      "\n",
      "Narration 8: C rides the bicycle along the road. .\n",
      "Timestamps: [55.77895683333333, 65.89524316666666]\n",
      "\n",
      "Narration 9: C rides the bicycle along the road. .\n",
      "Timestamps: [99.10580683333332, 109.22209316666668]\n",
      "\n",
      "Narration 10: C turns the bicycle left. .\n",
      "Timestamps: [132.04530683333334, 142.16159316666668]\n",
      "\n",
      "Narration 11: C rides the bicycle along the road. .\n",
      "Timestamps: [176.19205683333334, 186.30834316666667]\n",
      "\n",
      "Narration 12: C turns the bicycle right. .\n",
      "Timestamps: [243.4206868333333, 253.53697316666663]\n",
      "\n",
      "Narration 13: C rides the bicycle along the road. .\n",
      "Timestamps: [277.39018683333336, 287.50647316666664]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "# Load the narrations file\n",
    "with open('data/em_train_narrations.pkl', 'rb') as f:\n",
    "    clip_narrations = pickle.load(f)\n",
    "\n",
    "print(f\"Total number of clips: {len(clip_narrations)}\\n\")\n",
    "\n",
    "# Pick a random clip \n",
    "clip = random.choice(clip_narrations) \n",
    "\n",
    "print(f\"Clip ID: {clip['clip_uid']}\")\n",
    "print(f\"Narration Pass: {clip['narration_pass']}\")\n",
    "print(f\"Number of narrations in this clip: {len(clip['narrations'])}\\n\")\n",
    "\n",
    "# Print the narrations and their timestamps\n",
    "for i, narration in enumerate(clip['narrations']):\n",
    "    print(f\"Narration {i+1}: {narration['narration_text']}\")\n",
    "    print(f\"Timestamps: {narration['timestamps']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73945d8b",
   "metadata": {},
   "source": [
    "## Question Generation from Narrations\n",
    "\n",
    "In this snippet, we demonstrate how a **single narration** is used to generate a **question-answer pair** using the **LLaMA-2 13B model**.  \n",
    "\n",
    "**Key steps in the snippet:**\n",
    "1. The model and tokenizer are loaded on **GPU** with **FP16** precision for faster inference.  \n",
    "2. A **prompt** is prepared asking the model to generate a QA pair in **JSON format**, with constraints on the length of the question (≤10 words) and answer (≤5 words).  \n",
    "3. A single narration from one video clip is fed into the model, and the output is parsed into a JSON QA pair.\n",
    "\n",
    "**Note:**  \n",
    "- In this example, we only generate a QA pair for one narration.  \n",
    "- In practice, the same process is applied to **all narrations in the `em_train_narrations.pkl` file**.  \n",
    "- In the lab, this is done efficiently using **multiple GPUs** by leveraging the `generate_open_qa.py` and `merge.py` script in the `utils/` directory of the repository, which handles batching, parallel processing, and merging of generated QA pairs for the full dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c526b426",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eashwar/miniconda3/envs/info/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/eashwar/miniconda3/envs/info/lib/python3.9/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:18<00:00,  6.23s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from ast import literal_eval\n",
    "import torch\n",
    "\n",
    "# ---------------------------\n",
    "# Load model and tokenizer\n",
    "# ---------------------------\n",
    "model_id = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float32\n",
    ")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Ensure padding tokens are set\n",
    "pipe.tokenizer.pad_token = pipe.tokenizer.eos_token\n",
    "pipe.model.config.pad_token_id = pipe.model.config.eos_token_id\n",
    "\n",
    "# ---------------------------\n",
    "# Example narration\n",
    "# ---------------------------\n",
    "narration_text = \"C pours hot water into the bowl and stirs it with a spoon.\"\n",
    "\n",
    "# Prompt for question generation\n",
    "prompt = f\"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "You are an AI assistant. Generate one QA pair in JSON format: {{\"Q\": <question>, \"A\": <answer>}} \n",
    "based on the following narration. The question should be in past tense, within 10 words. \n",
    "The answer should be concise, within 5 words. 'C' is the person performing the actions.\n",
    "<</SYS>>\n",
    "\n",
    "<s>[INST] {narration_text} [/INST]\n",
    "\"\"\"\n",
    "\n",
    "# ---------------------------\n",
    "# Generate QA\n",
    "# ---------------------------\n",
    "output = pipe(prompt, max_new_tokens=64, do_sample=True, temperature=0.5, return_full_text=False)\n",
    "qa_pair = literal_eval(output[0][\"generated_text\"])\n",
    "\n",
    "print(\"Generated QA pair:\")\n",
    "print(qa_pair)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f03f063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Load model and tokenizer\n",
    "# ---------------------------\n",
    "# model_id = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     device_map=\"auto\",\n",
    "#     torch_dtype=torch.float32\n",
    "# )\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "pipe.tokenizer.pad_token = pipe.tokenizer.eos_token\n",
    "pipe.model.config.pad_token_id = pipe.model.config.eos_token_id\n",
    "\n",
    "# ---------------------------\n",
    "# Example QA pair\n",
    "# ---------------------------\n",
    "question = \"What did I pour in the bowl?\"\n",
    "answer = \"boiling water\"\n",
    "\n",
    "# ---------------------------\n",
    "# Prompt for generating wrong answers\n",
    "# ---------------------------\n",
    "prompt = f\"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "I'll provide a question and its correct answer. Generate three plausible, but incorrect, answers that closely resemble the correct one. Make it challenging to identify the right answer.\n",
    "<</SYS>>\n",
    "\n",
    "No preamble, get right to the three wrong answers and present them in a list format.\n",
    "Question: {question} Correct Answer: {answer}. Wrong Answers: [/INST]\n",
    "\"\"\"\n",
    "\n",
    "# ---------------------------\n",
    "# Generate wrong answers\n",
    "# ---------------------------\n",
    "output = pipe(\n",
    "    prompt,\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    temperature=0.5,\n",
    "    top_k=10,\n",
    "    return_full_text=False\n",
    ")\n",
    "\n",
    "wrong_answers = literal_eval(output[0][\"generated_text\"])\n",
    "print(\"Generated wrong answers:\")\n",
    "print(wrong_answers)\n",
    "\n",
    "# ---------------------------\n",
    "# Combine into final QA entry\n",
    "# ---------------------------\n",
    "qa_entry = {\n",
    "    \"question\": question,\n",
    "    \"answer\": answer,\n",
    "    \"wrong_answers\": wrong_answers\n",
    "}\n",
    "\n",
    "print(\"\\nFinal QA entry with wrong answers:\")\n",
    "print(qa_entry)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae55d82",
   "metadata": {},
   "source": [
    "## OpenQA and ClosedQA Annotation Generation\n",
    "\n",
    "In our project, we handle **both Open Question-Answering (OpenQA) and Closed Question-Answering (ClosedQA)**:\n",
    "\n",
    "- **OpenQA:** Generates a question-answer pair directly from the narrations, using the LLaMA model to produce a meaningful question and its corresponding answer. This forms the core of our question-answer dataset.  \n",
    "- **ClosedQA:** Takes an existing question and its correct answer, and generates **three plausible but incorrect answers**. These wrong answers are designed to be challenging and closely resemble the correct one, which is useful for training models that require multiple-choice style questions.  \n",
    "\n",
    "The process is applied to all narrations across all video clips in the dataset. After running the pipeline for all files, the final output is stored as **`annotations.EgoTimeQA.json`** in the `data/` directory of the repository.  \n",
    "\n",
    "This JSON file contains:\n",
    "- The question generated from narrations.  \n",
    "- The correct answer.  \n",
    "- Three incorrect but plausible wrong answers.  \n",
    "- Metadata such as video ID and timestamps for temporal grounding.  \n",
    "\n",
    "This combined OpenQA + ClosedQA dataset serves as the foundation for training and evaluation in our task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f03e56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfc27a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaec73a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "info",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
